import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical


#When you need to customize what fit() does, you should override the training step function of the Model class. 
#This is the function that is called by fit() for every batch of data. 
#You will then be able to call fit() as usual -- and it will be running your own learning algorithm.
#Note that this pattern does not prevent you from building models with the Functional API. 
#You can do this whether you're building Sequential models, Functional API models, or subclassed models.




# Custom Model with Newton's Method
class NewtonOptimizedModel(Model): #subclass model class
    def __init__(self):
    #By subclassing the Model class: in that case, you should define your layers in __init__() and you should implement the model's forward pass in call().
    #More Information: https://keras.io/api/models/model/

        super(NewtonOptimizedModel, self).__init__()
        self.learning_rate =0.001 #Change
        self.epsilon = 1e-3 #Change
### Which other hyperparameters of the model? -> test_size, random_state, metrics, params from model.fit (epochs=100, verbose=1, validation_split=0.2)), etc.. ?    
#### WEclhe ergeben sinn? (David)
        self.batch_size = X_train.shape[0] #DO NOT CHANGE
        
        
        # Define layers in a list for flexibility
        self.layers_list = [
            Dense(8, activation='relu', input_shape=(4,)),  # First layer
            Dense(8, activation='relu'),  # Second layer
            Dense(3, activation='softmax')   # Output layer
        ]
        
### Include dropout rate into Network as Hyperparameter? ###David fragen
#### Warum ja/ Warum nein? - intuition verstehen (Beide)
    # Potential Bias in Hessian:
    # The concern about the Hessian being "biased" with dropout arises from the fact that the Hessian is computed based on a randomly "thinned" network in each iteration. This means the Hessian you compute at each step does not represent the full network but a random subset of it.
    # As a result, the Hessian matrix might not accurately reflect the curvature of the loss landscape of the full network. This could lead to suboptimal or even misleading update steps in the optimization process.
         
    def call(self, inputs): #defines forward pass through the layers
        x = inputs
        for layer in self.layers_list:
            x = layer(x)
        return x
      

### In which cases can it be that matrices are asymmetric?
    #Typically the Hessian is symmetric 
        #only exception: Non-Smooth Functions
### Necessary to define the case of non Smooth functions in the standard code? (or just include in README?)
#### TO DO: Statt eigener Methode: Print -> Benutzer kann selbst modifizieren   (David fragen)  

    def asymmetric_case(self, h_mat):
        return 0.5 * (h_mat + tf.transpose(h_mat))

    def train_step(self, data): #override standard train_step method
        x, y = data #what is y?

        with tf.GradientTape(persistent=True) as t2: #outer tape t2 computes hessian; marked as persistent as used twice: For Hessian and Gradient
            with tf.GradientTape() as t1:
                y_pred = self(x, training=True) #Forward pass: layer 1 & 2 (here as x)
                loss = self.compiled_loss(y, y_pred) #Why not self.compute_loss which wraps the loss function that were passed compile()?
           
                # Apply Newton's method to each layer
            for layer in self.layers_list:
                if hasattr(layer, 'kernel'):
                    g = t1.gradient(loss, layer.kernel)
                    
                    if g is not None: # what if g is None?
                        #The Hessian is the Jacobian of the gradient, representing the second-order partial derivatives of the loss function.
                        h = t2.jacobian(g, layer.kernel) #The gradient of the loss with respect to the kernel of dense1 is computed.
                        
                        # Flatten out the axes of the Hessian and gradient
                        #The gradient and Hessian are reshaped into a vector and a matrix, respectively. This is necessary because the Hessian is a 4-dimensional tensor due to the shapes of the gradient and the kernel, but for Newton's method, we need to work with a matrix.
                        n_params = tf.reduce_prod(layer.kernel.shape)
                        g_vec = tf.reshape(g, [n_params, 1])
                        h_mat = tf.reshape(h, [n_params, n_params])
                       
                        # Newton's method update step #Newton's method update step is applied: the update is computed by solving a linear system where the Hessian matrix is modified by adding a small value (eps) to its diagonal to ensure numerical stability.
                        #In the standard Newton's method, the update step is computed as H^-1 g where H is the Hessian matrix and g is the gradient vector. 
                        #However, the Hessian might be singular (non-invertible) or nearly singular, which can cause numerical instability.
                        
                        #To ensure stability, a small value (eps, here 1e-3) is added to the diagonal of the Hessian matrix. This technique, known as regularization or damping, makes the Hessian matrix more stable for inversion.
                        eps = self.epsilon
                        eye_eps = tf.eye(h_mat.shape[0]) * eps # creates a diagonal matrix with eps along the diagonal and adds it to the Hessian
                        update = tf.linalg.solve(h_mat + eye_eps, g_vec) #tf.linalg.solve is then used to solve the linear system (h_mat + eye_eps) * update = g_vec, effectively computing H^-1 g for the update.
                        
                        # Reshape the update and apply it to the variable
                        layer.kernel.assign_sub(tf.reshape(update, layer.kernel.shape)) #The update calculated from Newton's method is subtracted from the kernel of self.dense1 using assign_sub. This adjusts the weights of the model based on the computed update.
                        #The update step's length is implicitly determined by the inverse of the modified Hessian matrix. In Newton's method, the step length is intrinsically linked to how the inverse Hessian scales the gradient.
 
#### Parallel optimization or sequential per layer? (David fragen)

### When is g None?
    # Non-Differentiable Functions: If the loss function includes non-differentiable operations, like absolute value, sign function, or ReLU (Rectified Linear Unit) at zero, gradients cannot be defined at certain points.
    # Discrete Operations: Operations that involve discretization or rounding (like floor or ceiling functions) do not have gradients.
    # Discontinuous Functions: Any discontinuity in the loss function prevents the existence of a gradient at that point.
    # Piecewise Defined Functions: If the loss function is piecewise defined and not smooth at the joints, gradients may not exist at those points.  
          
#### David fragen wie vorgegangen werden soll
    
### Is it rational to approximate? If so, how?

                    if g is None:
                            # Approximation des Gradienten mit tfp.math.value_and_gradient => finite difference method
                        g = tfp.math.value_and_gradient(lambda var: self.compiled_loss(y, self(x)), self.dense1.kernel)[1]
            
                        h = t2.jacobian(g, self.dense1.kernel)
            
                        n_params = tf.reduce_prod(self.dense1.kernel.shape)
                        g_vec = tf.reshape(g, [n_params, 1])
                        h_mat = tf.reshape(h, [n_params, n_params])
            
                        is_symmetric = tf.reduce_all(tf.equal(h_mat, tf.transpose(h_mat)))
                        # ist eine if then klausel in tensorflow
                        #https://stackoverflow.com/questions/35833011/how-to-add-if-condition-in-a-tensorflow-graph
                        h_mat = tf.cond(is_symmetric, lambda: self.asymmetric_case(h_mat), lambda: h_mat)
            
                        eye_eps = tf.eye(h_mat.shape[0]) * self.epsilon
                        
                        update = tf.linalg.solve(h_mat + eye_eps, g_vec)
                        self.dense1.kernel.assign_sub(tf.reshape(update, self.dense1.kernel.shape))                        

### Can Hessian be None and g is not None? If so, is it rational to approximate? If so, how?
    # Second Order Discontinuities: A function can be first-order differentiable (having a gradient) but lack continuous second-order derivatives (no Hessian) at certain points.
    # Corner or Cusp Points: At points where the function makes a sharp turn (like the absolute value function at zero), the Hessian does not exist even though the gradient does.
    # Gradient Not Differentiable Everywhere: There can be cases where the gradient is defined everywhere but is not differentiable at some points, thus lacking a Hessian at those points.
    
                
                del t2   #The persistent tape (t2) is deleted to free up resources.
                self.compiled_metrics.update_state(y, y_pred) # why not metric.update_state(y, y_pred) on metrics from self.metrics, 
                                                              #to update the state of the metrics that were passed in compile(), and we query results from self.metrics at the end to retrieve their current value?
                return {m.name: m.result() for m in self.metrics}
                #The model's metrics are updated with the predictions and actual labels, and the metrics' results are returned.



#### which dataset to benchmark? (David fragen)
#### was machen mit unit tests?




# Load dataset
file_path = '/path_to/iris.csv'
data = pd.read_csv(file_path)

# Preprocess data
X = data.iloc[:, 0:4].values
y = data.iloc[:, 4].values

# Encode class values as integers
encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)

# Convert integers to dummy variables (i.e. one hot encoded)
dummy_y = to_categorical(encoded_Y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    dummy_y, 
                                                    test_size=0.2, 
                                                    random_state=42)

# Create and compile model
model = NewtonOptimizedModel()
model.compile(loss='categorical_crossentropy', 
              metrics=['accuracy'])
#Wich other metricss?

# Set batch size
batch_size = X_train.shape[0]

# Train model
model.fit(X_train, 
          y_train, 
          batch_size=batch_size, 
          epochs=100, 
          verbose=1, 
          validation_split=0.2)

# Evaluate the model
scores = model.evaluate(X_test, y_test, batch_size=batch_size, verbose="auto")
print(f"Accuracy: {scores[1]*100}")

#General To Do:
    #1. Build Model
    #2. Test
    #3. Document
    #4. Run Benchmarks
    #5. Sub-Class Method


#Helpful Links: 
    #Newtons method:
        #https://www.youtube.com/watch?v=28BMpgxn_Ec
        #https://www.youtube.com/watch?v=uNDVfRq59Co
        #https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_4_Newtons.html
        #https://mmcryptogem.medium.com/training-a-neural-network-using-newtons-method-d02da8843133
    #Keras
        #https://keras.io/getting_started/intro_to_keras_for_engineers/
