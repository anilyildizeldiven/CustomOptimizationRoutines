import matplotlib.pyplot as plt
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from tensorflow.keras.initializers import RandomNormal, Constant

#Block 1
class NewtonOptimizedModel(Model): 
#Logistic Regression (No hidden Layer)
    #def __init__(self):
    #    super(NewtonOptimizedModel, self).__init__()
        # Only one layer for logistic regression. Assuming output for 3 classes.
    #    self.output_layer = Dense(3, activation='softmax', input_shape=(4,))

    #def call(self, inputs):
    #    return self.output_layer(inputs)
    
# One Layer
    def __init__(self):
        super(NewtonOptimizedModel, self).__init__()
        self.dense = Dense(15, activation='relu', input_shape=(13,))
        self.output_layer = Dense(3, activation='softmax')

    def call(self, inputs):
        x = self.dense(inputs)
        return self.output_layer(x)



# Block 2: Modified Train Step with Block-Diagonal Hessian Approximation
def train_step(self, data):
    x, y = data

    with tf.GradientTape(persistent=True) as t2:  
        with tf.GradientTape(persistent=True) as tape:  
            y_pred = self(x, training=True)
            loss = self.compiled_loss(y, y_pred)

    # Compute gradients for all layers
            grads = tape.gradient(loss, self.trainable_variables)

    # Hessian calculation for each layer
    hessians = {}
    for grad, var in zip(grads, self.trainable_variables):
        # Compute Hessian for this layer
        hessian = t2.jacobian(grad, var)
        hessian = tf.reshape(hessian, [tf.size(var), tf.size(var)])  # Reshape to 2D
        hessians[var.name] = hessian

    # Regularize and invert Hessian blocks
    eps = 1e-4  # Regularization term
    for var in self.trainable_variables:
        hessian_block = hessians[var.name]
        # Regularize the Hessian block
        hessian_reg = hessian_block + tf.eye(tf.shape(hessian_block)[0]) * eps
        # Invert the regularized Hessian block
        inv_hessian_block = tf.linalg.inv(hessian_reg)

        # Compute update using the inverse Hessian block
        grad = tape.gradient(loss, var)
        update = tf.linalg.matmul(inv_hessian_block, tf.reshape(grad, [-1, 1]))
        var.assign_sub(tf.reshape(update, var.shape))

    del tape, t2
    self.compiled_metrics.update_state(y, y_pred)
    return {m.name: m.result() for m in self.metrics}


    




#Block 6   
    
# Load the wine dataset
file_path = '/path_to/wine.csv'
data = pd.read_csv(file_path)

# Assuming the target variable is in the first column
X = data.iloc[:, 1:].values
y = data.iloc[:, 0].values

# Encode class values as integers
encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)

# Convert integers to dummy variables (one hot encoded)
dummy_y = to_categorical(encoded_Y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.25, random_state=42)

#Block 7

    
    
class StoreWeightNormCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(StoreWeightNormCallback, self).__init__()
        self.weight_norms_per_epoch = []

    def on_epoch_end(self, epoch, logs=None):
        weights = self.model.get_weights()
        weight_norms = [np.linalg.norm(w) for w in weights]
        self.weight_norms_per_epoch.append(weight_norms)
        print(f"Epoch {epoch + 1}, Weight norms: {weight_norms}")
        
#Block 8
        
# Adjust the model architecture as necessary based on the wine dataset features
model = NewtonOptimizedModel()
model.compile(loss='categorical_crossentropy', metrics=['accuracy'])

# Batch size and training process
batch_size = X_train.shape[0]
callback = StoreWeightNormCallback()

model.fit(X_train, y_train, batch_size=batch_size, epochs=100, validation_split=0.2, callbacks=[callback])




#Block 9

# Access the stored weight norms
epoch_weight_norms = callback.weight_norms_per_epoch
print(epoch_weight_norms)

# Evaluate the model
scores = model.evaluate(X_test, y_test, batch_size=batch_size, verbose="auto")
print(f"Accuracy: {scores[1]*100}")

