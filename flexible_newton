import matplotlib.pyplot as plt
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from tensorflow.keras.initializers import RandomNormal, Constant

#Block 1
class NewtonOptimizedModel(Model): 
#Logistic Regression (No hidden Layer)
    # def __init__(self):
    #     super(NewtonOptimizedModel, self).__init__()
    #     # Only one layer for logistic regression. Assuming output for 3 classes.
    #     self.output_layer = Dense(3, activation='softmax', input_shape=(4,))

    # def call(self, inputs):
    #     return self.output_layer(inputs)
    
    
    def __init__(self):
        super(NewtonOptimizedModel, self).__init__()
        self.dense = Dense(15, activation='relu', input_shape=(13,))
        self.output_layer = Dense(3, activation='softmax')

    def call(self, inputs):
        x = self.dense(inputs)
        return self.output_layer(x)


# Block 2
def train_step(self, data):
    x, y = data

    with tf.GradientTape() as tape:
        with tf.GradientTape(persistent=True) as t2:
            y_pred = self(x, training=True)
            loss = self.compiled_loss(y, y_pred)
            t2.watch(self.trainable_variables)
        grads = tape.gradient(loss, self.trainable_variables)

    # Compute the full Hessian
    hessian = []
    for g in grads:
        hessian_rows = []
        for v in self.trainable_variables:
            hessian_row = t2.jacobian(g, v)
            if hessian_row is None:
                # If the gradient is None, it implies that this part of the Hessian is zero
                hessian_row = tf.zeros_like(v)
            hessian_rows.append(tf.reshape(hessian_row, [-1]))
        hessian.append(tf.concat(hessian_rows, axis=0))
    full_hessian = tf.concat(hessian, axis=0)

    # Flatten gradients
    flat_grads = tf.concat([tf.reshape(g, [-1]) for g in grads], axis=0)

    # Regularize and invert Hessian
    eps = 1e-4  # Regularization term
    hessian_reg = full_hessian + tf.eye(tf.size(flat_grads)) * eps
    inv_hessian = tf.linalg.inv(hessian_reg)

    # Compute update using the inverse Hessian
    update = tf.linalg.matmul(inv_hessian, tf.expand_dims(flat_grads, 1))
    update = tf.split(update, [tf.size(v) for v in self.trainable_variables])

    # Apply updates to each variable
    for var, u in zip(self.trainable_variables, update):
        var.assign_sub(tf.reshape(u, var.shape))

    del tape, t2
    self.compiled_metrics.update_state(y, y_pred)
    return {m.name: m.result() for m in self.metrics}



#Block 3 
    
# Load the wine dataset
file_path = '/path_to/wine.csv'
data = pd.read_csv(file_path)

# Assuming the target variable is in the first column
X = data.iloc[:, 1:].values
y = data.iloc[:, 0].values

# Encode class values as integers
encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)

# Convert integers to dummy variables (one hot encoded)
dummy_y = to_categorical(encoded_Y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.25, random_state=42)

#Block 4

    
    
class StoreWeightNormCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(StoreWeightNormCallback, self).__init__()
        self.weight_norms_per_epoch = []

    def on_epoch_end(self, epoch, logs=None):
        weights = self.model.get_weights()
        weight_norms = [np.linalg.norm(w) for w in weights]
        self.weight_norms_per_epoch.append(weight_norms)
        print(f"Epoch {epoch + 1}, Weight norms: {weight_norms}")
        
#Block 5
        
# Adjust the model architecture as necessary based on the wine dataset features
model = NewtonOptimizedModel()
model.compile(loss='categorical_crossentropy', metrics=['accuracy'])

# Batch size and training process
batch_size = X_train.shape[0]
callback = StoreWeightNormCallback()

model.fit(X_train, y_train, batch_size=batch_size, epochs=100, validation_split=0.2, callbacks=[callback])




#Block 6

# Access the stored weight norms
epoch_weight_norms = callback.weight_norms_per_epoch
print(epoch_weight_norms)

# Evaluate the model
scores = model.evaluate(X_test, y_test, batch_size=batch_size, verbose="auto")
print(f"Accuracy: {scores[1]*100}")

